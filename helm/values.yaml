# Sample Service Configuration
# A microservice for SRE monitoring challenges

# Application Configuration
global:
  grafana_dashboard_base_url: "http://localhost:8085/d/a671feb6-2a49-40dd-a08e-a8e2546d5824d/sample-service-dashboard"

app:
  alerts:
    enabled: true
    rules:
      - name: SampleServiceDown
        expr: sample_service_up == 0
        for: 1m
        severity: critical
        annotations:
          summary: "Sample Service is down"
          description: "The sample service is reporting 'up=0' for more than 1 minute."
          grafana_dashboard: "{{ .Values.global.grafana_dashboard_base_url }}?orgId=1&from=now-15m&to=now&timezone=browser&editPanel=4"
      - name: SampleServiceHighErrorRate
        expr: sum(rate(sample_service_requests_total{status=~"5.."}[5m])) > (2 / 300)
        for: 1m
        severity: warning
        annotations:
          summary: "High error rate on Sample Service"
          description: "Error rate >5% over the last 5 minutes."
          grafana_dashboard: "{{ .Values.global.grafana_dashboard_base_url }}?orgId=1&from=now-15m&to=now&timezone=browser&editPanel=5"
      - name: SampleServiceHighLatency
        expr: histogram_quantile(
           0.95,
           sum(rate(sample_service_request_duration_seconds_bucket[5m])) by (le)
          ) > 0.1 
        for: 1m
        severity: warning
        annotations:
          summary: "High latency on Sample Service"
          description: "Latency >95th percentile over the last 1 minutes."
          grafana_dashboard: "{{ .Values.global.grafana_dashboard_base_url }}?orgId=1&from=now-15m&to=now&timezone=browser&editPanel=2"
      - name: ErrorBudgetBurnRate
        expr: (sum(rate(sample_service_requests_total{status=~"5.."}[5m])) / sum(rate(sample_service_requests_total[5m]))) > (0.01 * 2)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error budget burn rate"
          description: "Error budget is being burned too quickly (2x) â€” check appliaction"
          grafana_dashboard: "{{ .Values.global.grafana_dashboard_base_url }}?orgId=1&from=now-15m&to=now&timezone=browser&tab=queries&editPanel=7"

  name: sample-service
  # DockerHub image with latest improvements
  image: haymed/sample-service
  tag: latest
  imagePullPolicy: Always
  replicas: 1

# Resource Configuration
resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

# Health Checks
healthChecks:
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 5
    periodSeconds: 5

# Service Configuration
service:
  type: ClusterIP
  port: 80
  targetPort: 8080

# Tracing Configuration (configure your tracing backend)
tracing:
  # Set this to your Jaeger/OTLP collector endpoint
  # Example: "http://jaeger-collector:4318/v1/traces"
  otlpEndpoint: ""
  protocol: "http/protobuf"

# Application Environment Variables
env:
  - name: LOG_LEVEL
    value: "INFO"
  - name: LOG_FORMAT
    value: "json"
  # OpenTelemetry service identification (tracing available - configure your endpoint!)
  - name: OTEL_SERVICE_NAME
    value: "sample-service"
  - name: OTEL_SERVICE_VERSION
    value: "1.0.0" 
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://otel-opentelemetry-collector.obs:4318/v1/traces"